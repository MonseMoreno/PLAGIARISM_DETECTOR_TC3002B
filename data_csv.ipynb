{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "base_path = 'IR-Plag-Dataset'\n",
    "output_csv = 'dataset.csv'\n",
    "rows = []\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read().replace('\\n', '\\\\n').replace('\\r', '')\n",
    "\n",
    "for case in sorted(os.listdir(base_path)):\n",
    "    case_path = os.path.join(base_path, case)\n",
    "    if not os.path.isdir(case_path):\n",
    "        continue\n",
    "\n",
    "    dataset_name = 'IR-Plag'\n",
    "    original_path = os.path.join(case_path, 'original')\n",
    "    original_file = next(os.scandir(original_path)).path\n",
    "    original_code = read_file(original_file)\n",
    "    original_id = f'{case}-ORIG'\n",
    "\n",
    "    nonplag_path = os.path.join(case_path, 'non-plagiarized')\n",
    "    if os.path.exists(nonplag_path):\n",
    "        for folder in sorted(os.listdir(nonplag_path)):\n",
    "            folder_path = os.path.join(nonplag_path, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            file_path = next(os.scandir(folder_path)).path\n",
    "            code = read_file(file_path)\n",
    "            file_id = f'{case}-NP-{folder}'\n",
    "            rows.append([f'{original_id}_{file_id}', original_id, original_code, file_id, code, 0, dataset_name])\n",
    "\n",
    "    plag_path = os.path.join(case_path, 'plagiarized')\n",
    "    if os.path.exists(plag_path):\n",
    "        for level in sorted(os.listdir(plag_path)):\n",
    "            level_path = os.path.join(plag_path, level)\n",
    "            for folder in sorted(os.listdir(level_path)):\n",
    "                folder_path = os.path.join(level_path, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                file_path = next(os.scandir(folder_path)).path\n",
    "                code = read_file(file_path)\n",
    "                file_id = f'{case}-{level}-{folder}'\n",
    "                rows.append([f'{original_id}_{file_id}', original_id, original_code, file_id, code, 1, dataset_name])\n",
    "\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['id', 'idcode1', 'code1', 'idcode2', 'code2', 'result', 'dataset'])\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar conplag al CSV\n",
    "import pandas as pd\n",
    "\n",
    "conplag_base = 'conplag_version_2/versions'\n",
    "conplag_code_dir = os.path.join(conplag_base, 'version_2')\n",
    "conplag_labels = os.path.join(conplag_base, 'labels.csv')\n",
    "\n",
    "df = pd.read_csv(conplag_labels)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sub1 = row['sub1']\n",
    "    sub2 = row['sub2']\n",
    "    verdict = row['verdict']\n",
    "    codeid1 = str(sub1)\n",
    "    codeid2 = str(sub2)\n",
    "    folder = f'{codeid1}_{codeid2}'\n",
    "    folder_path = os.path.join(conplag_code_dir, folder)\n",
    "\n",
    "    file1_path = os.path.join(folder_path, f'{codeid1}.java')\n",
    "    file2_path = os.path.join(folder_path, f'{codeid2}.java')\n",
    "\n",
    "    if not os.path.exists(file1_path) or not os.path.exists(file2_path):\n",
    "        continue\n",
    "\n",
    "    code1 = read_file(file1_path)\n",
    "    code2 = read_file(file2_path)\n",
    "    pair_id = f'{codeid1}_{codeid2}'\n",
    "    result = int(verdict)\n",
    "    dataset_name = 'conplag_version_2'\n",
    "\n",
    "    rows.append([pair_id, codeid1, code1, codeid2, code2, result, dataset_name])\n",
    "\n",
    "with open(output_csv, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    for row in rows:\n",
    "        if row[-1] == 'conplag_version_2':\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "base_path = 'fire14-source-code-training-dataset'\n",
    "java_path = os.path.join(base_path, 'java')\n",
    "qrel_path = os.path.join(base_path, 'SOCO14-java.qrel')\n",
    "output_csv = 'dataset.csv'\n",
    "dataset_name = 'FIRE14'\n",
    "\n",
    "def read_code(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read().replace('\\n', '\\\\n').replace('\\r', '')\n",
    "\n",
    "# Cargar todos los nombres de archivos disponibles\n",
    "all_files = [f for f in os.listdir(java_path) if f.endswith('.java')]\n",
    "file_set = set(all_files)\n",
    "\n",
    "# Leer pares positivos del archivo QREL\n",
    "positive_pairs = set()\n",
    "with open(qrel_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            f1, f2 = parts[0], parts[1]\n",
    "            f1 = f1 if f1.endswith('.java') else f1 + '.java'\n",
    "            f2 = f2 if f2.endswith('.java') else f2 + '.java'\n",
    "            if f1 in file_set and f2 in file_set:\n",
    "                positive_pairs.add((f1, f2))\n",
    "\n",
    "# Generar pares negativos (aleatorios que no est√©n en positivos)\n",
    "positive_ids = list({f for pair in positive_pairs for f in pair})\n",
    "negatives = set()\n",
    "while len(negatives) < len(positive_pairs):\n",
    "    f1, f2 = random.sample(positive_ids, 2)\n",
    "    if f1 != f2 and (f1, f2) not in positive_pairs and (f2, f1) not in positive_pairs:\n",
    "        negatives.add((f1, f2))\n",
    "\n",
    "# Guardar ambos tipos en el CSV\n",
    "with open(output_csv, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    # Agregar positivos\n",
    "    for f1, f2 in positive_pairs:\n",
    "        path1 = os.path.join(java_path, f1)\n",
    "        path2 = os.path.join(java_path, f2)\n",
    "        if os.path.exists(path1) and os.path.exists(path2):\n",
    "            code1 = read_code(path1)\n",
    "            code2 = read_code(path2)\n",
    "            writer.writerow([f'{f1}_{f2}', f1, code1, f2, code2, 1, dataset_name])\n",
    "\n",
    "    # Agregar negativos\n",
    "    for f1, f2 in negatives:\n",
    "        path1 = os.path.join(java_path, f1)\n",
    "        path2 = os.path.join(java_path, f2)\n",
    "        if os.path.exists(path1) and os.path.exists(path2):\n",
    "            code1 = read_code(path1)\n",
    "            code2 = read_code(path2)\n",
    "            writer.writerow([f'{f1}_{f2}', f1, code1, f2, code2, 0, dataset_name])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
