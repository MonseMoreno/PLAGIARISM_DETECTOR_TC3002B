{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backports.tarfile in c:\\users\\pablo\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: zss in c:\\users\\pablo\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from zss) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install backports.tarfile\n",
    "%pip install zss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "base_path = 'IR-Plag-Dataset'\n",
    "pairs_dict = OrderedDict()\n",
    "\n",
    "for case in sorted(os.listdir(base_path)):\n",
    "    case_path = os.path.join(base_path, case)\n",
    "    if not os.path.isdir(case_path):\n",
    "        continue\n",
    "\n",
    "    dataset_name = 'IR-Plag'\n",
    "    orig_path = os.path.join(case_path, 'original')\n",
    "    orig_file = next(os.scandir(orig_path)).path\n",
    "    orig_id   = f'{case}-ORIG'\n",
    "\n",
    "    nonplag_path = os.path.join(case_path, 'non-plagiarized')\n",
    "    if os.path.exists(nonplag_path):\n",
    "        for folder in sorted(os.listdir(nonplag_path)):\n",
    "            folder_path = os.path.join(nonplag_path, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            file_path = next(os.scandir(folder_path)).path\n",
    "            file_id   = f'{case}-NP-{folder}'\n",
    "            pair_id   = f'{orig_id}_{file_id}'\n",
    "            pairs_dict[pair_id] = {\n",
    "                'idcode1': orig_id,\n",
    "                'code1'  : orig_file,\n",
    "                'idcode2': file_id,\n",
    "                'code2'  : file_path,\n",
    "                'result' : 0,\n",
    "                'dataset': dataset_name\n",
    "            }\n",
    "            \n",
    "            inverted_pair_id = f'{file_id}_{orig_id}'\n",
    "            pairs_dict[inverted_pair_id] = {\n",
    "                'idcode1': file_id,\n",
    "                'code1'  : file_path,\n",
    "                'idcode2': orig_id,\n",
    "                'code2'  : orig_file,\n",
    "                'result' : 0,\n",
    "                'dataset': dataset_name\n",
    "            }\n",
    "\n",
    "    plag_path = os.path.join(case_path, 'plagiarized')\n",
    "    if os.path.exists(plag_path):\n",
    "        for level in sorted(os.listdir(plag_path)):\n",
    "            level_path = os.path.join(plag_path, level)\n",
    "            for folder in sorted(os.listdir(level_path)):\n",
    "                folder_path = os.path.join(level_path, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                file_path = next(os.scandir(folder_path)).path\n",
    "                file_id   = f'{case}-{level}-{folder}'\n",
    "                pair_id   = f'{orig_id}_{file_id}'\n",
    "                pairs_dict[pair_id] = {\n",
    "                    'idcode1': orig_id,\n",
    "                    'code1'  : orig_file,\n",
    "                    'idcode2': file_id,\n",
    "                    'code2'  : file_path,\n",
    "                    'result' : 1,\n",
    "                    'dataset': dataset_name\n",
    "                }\n",
    "                \n",
    "                inverted_pair_id = f'{file_id}_{orig_id}'\n",
    "                pairs_dict[inverted_pair_id] = {\n",
    "                    'idcode1': file_id,\n",
    "                    'code1'  : file_path,\n",
    "                    'idcode2': orig_id,\n",
    "                    'code2'  : orig_file,\n",
    "                    'result' : 1,\n",
    "                    'dataset': dataset_name\n",
    "                }\n",
    "\n",
    "print(len(pairs_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total de pares tras añadir CONPLAG: 2742\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "conplag_base     = 'conplag_version_2/versions'\n",
    "conplag_code_dir = os.path.join(conplag_base, 'version_2')\n",
    "conplag_labels   = os.path.join(conplag_base, 'labels.csv')\n",
    "\n",
    "df = pd.read_csv(conplag_labels)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sub1, sub2, verdict = row['sub1'], row['sub2'], int(row['verdict'])\n",
    "\n",
    "    codeid1  = str(sub1)\n",
    "    codeid2  = str(sub2)\n",
    "    pair_id  = f'{codeid1}_{codeid2}'\n",
    "\n",
    "    folder_path = os.path.join(conplag_code_dir, pair_id)\n",
    "    file1_path  = os.path.join(folder_path, f'{codeid1}.java')\n",
    "    file2_path  = os.path.join(folder_path, f'{codeid2}.java')\n",
    "\n",
    "    if not (os.path.exists(file1_path) and os.path.exists(file2_path)):\n",
    "        continue\n",
    "\n",
    "    pairs_dict[pair_id] = {\n",
    "        'idcode1': codeid1,\n",
    "        'code1'  : file1_path,          \n",
    "        'idcode2': codeid2,\n",
    "        'code2'  : file2_path,          \n",
    "        'result' : verdict,             \n",
    "        'dataset': 'conplag_version_2'\n",
    "    }\n",
    "    \n",
    "    inverted_pair_id = f'{codeid2}_{codeid1}'\n",
    "    pairs_dict[inverted_pair_id] = {\n",
    "        'idcode1': codeid2,\n",
    "        'code1'  : file2_path,          \n",
    "        'idcode2': codeid1,\n",
    "        'code2'  : file1_path,          \n",
    "        'result' : verdict,             \n",
    "        'dataset': 'conplag_version_2'\n",
    "    }\n",
    "\n",
    "print(f' Total de pares tras añadir CONPLAG: {len(pairs_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total de pares tras añadir FIRE14: 3078\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "\n",
    "base_path   = 'fire14-source-code-training-dataset'\n",
    "java_path   = os.path.join(base_path, 'java')\n",
    "qrel_path   = os.path.join(base_path, 'SOCO14-java.qrel')\n",
    "dataset_name = 'FIRE14'\n",
    "\n",
    "all_files = [f for f in os.listdir(java_path) if f.endswith('.java')]\n",
    "file_set  = set(all_files)\n",
    "\n",
    "positive_pairs = set()\n",
    "with open(qrel_path, encoding='utf-8') as fh:\n",
    "    for line in fh:\n",
    "        f1, f2 = line.split()[:2]\n",
    "        if not f1.endswith('.java'): f1 += '.java'\n",
    "        if not f2.endswith('.java'): f2 += '.java'\n",
    "        if f1 in file_set and f2 in file_set:\n",
    "            positive_pairs.add((f1, f2))\n",
    "\n",
    "positive_ids = list({x for p in positive_pairs for x in p})\n",
    "negatives = set()\n",
    "while len(negatives) < len(positive_pairs):\n",
    "    f1, f2 = random.sample(positive_ids, 2)\n",
    "    if f1 != f2 and (f1, f2) not in positive_pairs and (f2, f1) not in positive_pairs:\n",
    "        negatives.add((f1, f2))\n",
    "\n",
    "def add_pair(f1, f2, label):\n",
    "    p1, p2 = os.path.join(java_path, f1), os.path.join(java_path, f2)\n",
    "    if not (os.path.exists(p1) and os.path.exists(p2)): return\n",
    "    pair_id = f'{f1}_{f2}'\n",
    "    pairs_dict[pair_id] = {\n",
    "        'idcode1': f1,\n",
    "        'code1'  : p1,\n",
    "        'idcode2': f2,\n",
    "        'code2'  : p2,\n",
    "        'result' : label,\n",
    "        'dataset': dataset_name\n",
    "    }\n",
    "    \n",
    "    inverted_pair_id = f'{f2}_{f1}'\n",
    "    pairs_dict[inverted_pair_id] = {\n",
    "        'idcode1': f2,\n",
    "        'code1'  : p2,\n",
    "        'idcode2': f1,\n",
    "        'code2'  : p1,\n",
    "        'result' : label,\n",
    "        'dataset': dataset_name\n",
    "    }\n",
    "\n",
    "for f1, f2 in positive_pairs: add_pair(f1, f2, 1)\n",
    "for f1, f2 in negatives:      add_pair(f1, f2, 0)\n",
    "\n",
    "print(f' Total de pares tras añadir FIRE14: {len(pairs_dict)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code(file_path):\n",
    "    with open(file_path, encoding='utf-8') as fh:\n",
    "        return fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parseando AST (vocab):  83%|████████▎ | 2557/3078 [00:23<00:05, 94.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  AST skip conplag_version_2/versions\\version_2\\3d06b643_db7f80a5\\db7f80a5.java  ()\n",
      "⚠️  AST skip conplag_version_2/versions\\version_2\\3d06b643_db7f80a5\\db7f80a5.java  ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parseando AST (vocab): 100%|██████████| 3078/3078 [00:27<00:00, 111.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, javalang\n",
    "from javalang import tree as jt \n",
    "import numpy as np\n",
    "from simhash import Simhash\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "_token_pat = re.compile(r'[A-Za-z_]\\w+|\\d+|==|!=|<=|>=|&&|\\|\\||[^\\sA-Za-z0-9_]')\n",
    "\n",
    "# Funciones para calcular indices de similitud\n",
    "def clean_code(src:str)->str:\n",
    "    \n",
    "    src = re.sub(r'//.*?$|/\\*.*?\\*/', '', src, flags=re.S|re.M)  # comentarios\n",
    "    return '\\n'.join(line.strip() for line in src.splitlines() if line.strip())\n",
    "\n",
    "def text_features(src:str):\n",
    "    \n",
    "    tokens = _token_pat.findall(clean_code(src))\n",
    "    freq   = Counter(tokens)\n",
    "    for tok, w in freq.items():\n",
    "        yield tok, w                    \n",
    "\n",
    "def simhash_text(src:str)->Simhash:\n",
    "    return Simhash(list(text_features(src)))\n",
    "\n",
    "def text_similarity(code_a:str, code_b:str)->float:\n",
    "    h1, h2 = simhash_text(code_a), simhash_text(code_b)\n",
    "    return 1 - h1.distance(h2) / 64.0\n",
    "\n",
    "def walk_ast(node):\n",
    "    yield node.__class__.__name__\n",
    "    for child in node.children:\n",
    "        if isinstance(child, list):\n",
    "            for g in child:\n",
    "                if isinstance(g, javalang.ast.Node):\n",
    "                    yield f'{node.__class__.__name__}->{g.__class__.__name__}'\n",
    "                    yield from walk_ast(g)\n",
    "        elif isinstance(child, javalang.ast.Node):\n",
    "            yield f'{node.__class__.__name__}->{child.__class__.__name__}'\n",
    "            yield from walk_ast(child)\n",
    "\n",
    "def simhash_ast(src:str)->Simhash:\n",
    "    ast   = javalang.parse.parse(src)\n",
    "    feats = list(walk_ast(ast))\n",
    "    return Simhash(feats)\n",
    "\n",
    "def ast_similarity(code_a:str, code_b:str)->float:\n",
    "    h1, h2 = simhash_ast(code_a), simhash_ast(code_b)\n",
    "    return 1 - h1.distance(h2) / 64.0\n",
    "\n",
    "# Funciones para crear cadena de Markov\n",
    "def iter_children(node):\n",
    "    for ch in node.children:\n",
    "        if isinstance(ch, jt.Node):\n",
    "            yield ch\n",
    "        elif isinstance(ch, (list, tuple)):\n",
    "            for g in ch:\n",
    "                if isinstance(g, jt.Node):\n",
    "                    yield g\n",
    "\n",
    "def ast_to_seq(node):\n",
    "    \n",
    "    seq, stack = [], [node]\n",
    "    while stack:\n",
    "        n = stack.pop()\n",
    "        seq.append(type(n).__name__)        \n",
    "        stack.extend(reversed(list(iter_children(n))))\n",
    "    return seq\n",
    "\n",
    "def build_vocab(ast_objs):\n",
    "    vocab = set()\n",
    "    for ast in ast_objs:\n",
    "        vocab.update(ast_to_seq(ast))\n",
    "    return {tok: i for i, tok in enumerate(sorted(vocab))}\n",
    "\n",
    "def transition_matrix(seq, vocab):\n",
    "    n  = len(vocab)\n",
    "    M  = [[0]*n for _ in range(n)]\n",
    "    idx = [vocab[t] for t in seq]\n",
    "    for i in range(len(idx)-1):\n",
    "        M[idx[i]][idx[i+1]] += 1\n",
    "    \n",
    "    for r in range(n):\n",
    "        s = sum(M[r])\n",
    "        if s: M[r] = [x/s for x in M[r]]\n",
    "    return M          \n",
    "\n",
    "SPACE = \" \"\n",
    "\n",
    "def mat_to_str(mat: np.ndarray) -> str:\n",
    "    \n",
    "    return SPACE.join(f\"{x:.6f}\" for x in mat.ravel())\n",
    "\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def parse_ast(path:str):\n",
    "    code = Path(path).read_text(encoding='utf-8', errors='ignore')\n",
    "    return javalang.parse.parse(code)\n",
    "\n",
    "all_asts = []\n",
    "for info in tqdm(pairs_dict.values(), total=len(pairs_dict),\n",
    "                 desc=\"Parseando AST (vocab)\"):\n",
    "     # codigo 1\n",
    "    try:\n",
    "        all_asts.append(parse_ast(info[\"code1\"]))\n",
    "    except Exception as e:\n",
    "        print(f\"  AST skip {info['code1']}  ({e})\")\n",
    "\n",
    "    # codigo 2\n",
    "    try:\n",
    "        all_asts.append(parse_ast(info[\"code2\"]))\n",
    "    except Exception as e:\n",
    "        print(f\"  AST skip {info['code2']}  ({e})\")\n",
    "\n",
    "VOCAB = build_vocab(all_asts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando pares:  83%|████████▎ | 2553/3078 [00:24<00:05, 100.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  3d06b643_db7f80a5 omitido ()\n",
      "⚠️  db7f80a5_3d06b643 omitido ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando pares: 100%|██████████| 3078/3078 [00:32<00:00, 96.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  dataset.csv generado con 3076 filas\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "total = len(pairs_dict)\n",
    "\n",
    "for pid, info in tqdm(pairs_dict.items(),\n",
    "                      total=total,\n",
    "                      desc=\"Procesando pares\"):\n",
    "    try:\n",
    "        # Codigo y ast\n",
    "        code_a = read_code(info[\"code1\"])\n",
    "        code_b = read_code(info[\"code2\"])\n",
    "        ast_a  = parse_ast(info[\"code1\"])\n",
    "        ast_b  = parse_ast(info[\"code2\"])\n",
    "\n",
    "        # Similitudes\n",
    "        simhash_sim = text_similarity(code_a, code_b)\n",
    "        sim_ast = ast_similarity(code_a, code_b)\n",
    "\n",
    "        # Markov\n",
    "        # flat y serializacion para CSV\n",
    "        M1 = transition_matrix(ast_to_seq(ast_a), VOCAB)\n",
    "        M2 = transition_matrix(ast_to_seq(ast_b), VOCAB)\n",
    "        M1 = np.asarray(M1, dtype=np.float32)\n",
    "        M2 = np.asarray(M2, dtype=np.float32)\n",
    "\n",
    "        rows.append({\n",
    "            \"id\"           : pid,\n",
    "            \"idcode1\"      : info[\"idcode1\"],\n",
    "            \"idcode2\"      : info[\"idcode2\"],\n",
    "            \"code1\"        : info[\"code1\"],   # ruta\n",
    "            \"code2\"        : info[\"code2\"],   # ruta\n",
    "            \"simhash\"      : simhash_sim,\n",
    "            \"astsimilarity\": sim_ast,\n",
    "            \"markov1\"      : mat_to_str(M1),\n",
    "            \"markov2\"      : mat_to_str(M2),\n",
    "            \"dataset\"      : info[\"dataset\"],\n",
    "            \"result\"       : info[\"result\"],\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        # Cualquier fallo de parseo/similaridad ⇒ se omite el par\n",
    "        print(f\"  {pid} omitido ({e})\")\n",
    "\n",
    "# Guarda el CSV\n",
    "import pandas as pd\n",
    "pd.DataFrame(rows).to_csv(\"dataset.csv\", index=False)\n",
    "print(\"  dataset.csv generado con\", len(rows), \"filas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
