{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrOoWDuXzlA4"
      },
      "source": [
        "Código reutilizado de la actividad 4.4  | Modulo Métodos cuantitavos y Simulación.                             \n",
        "\n",
        "Agregar Columnas de calculo BOW, TF-IDF y Markov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCJ1V9yszjoL",
        "outputId": "e19094b5-3eaa-4371-f2c6-321bf16aadd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV guardado -> dataset_Similarity_calculation.csv\n",
            "Primera fila:\n",
            "BoW   : 0.706597242747782\n",
            "TF-IDF: 0.6451639235437321\n",
            "Markov A: \\npublic class T1 {\\n\tpublic static void main(String[] args) {\\n\t\tSystem.out.println(\"Welcome to Java\");\\n\t\tSystem.out.println(\"Welcome to Java\");\\n\t\tSystem.out.println(\"Welcome to Java\");\\n\t\tSystem.out.println(\"Welcome to Java\");\\n\t\tSystem.out.println(\"Welcome to Java\");\\n\t}\\n\\n}\\n\n",
            "Markov vector A: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.35714285714285715, 0.0, 0.0, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.0, 0.35714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.5555555555555556, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2727272727272727, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6363636363636364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.38461538461538464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.38461538461538464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.45454545454545453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45454545454545453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045454545454545456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13636363636363635, 0.0, 0.09090909090909091, 0.0, 0.2727272727272727, 0.22727272727272727, 0.0, 0.0, 0.22727272727272727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.3125, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 0.0, 0.0, 0.0, 0.06666666666666667, 0.5333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.041666666666666664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0, 0.0, 0.20833333333333334, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0, 0.20833333333333334, 0.0, 0.0, 0.20833333333333334, 0.20833333333333334, 0.041666666666666664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Markov B: /*\\n * To change this license header, choose License Headers in Project Properties.\\n * To change this template file, choose Tools | Templates\\n * and open the template in the editor.\\n */\\n\\n/**\\n *\\n * @author CB6AB3315634A1E4D11B091BA48B60BA 7E51EEA5FA101ED4DADE9AD3A7A072BB 2F809B10D1ABEDBF2EC288F851B3EBBB\\n */\\npublic class T01 {\\n    public static void main(String[] args){\\n        \\n        for(int i = 0; i < 5; i++){\\n            System.out.println(\"Welcome To Java\");\\n        }\\n        \\n    }\\n}\\n\n",
            "Markov vector B: [0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.125, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.08333333333333333, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038461538461538464, 0.07692307692307693, 0.0, 0.0, 0.038461538461538464, 0.038461538461538464, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.038461538461538464, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.038461538461538464, 0.038461538461538464, 0.15384615384615385, 0.038461538461538464, 0.038461538461538464, 0.0, 0.0, 0.058823529411764705, 0.058823529411764705, 0.058823529411764705, 0.11764705882352941, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.11764705882352941, 0.17647058823529413, 0.0, 0.0, 0.058823529411764705, 0.058823529411764705, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.058823529411764705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.07142857142857142, 0.0, 0.14285714285714285, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.07142857142857142, 0.07142857142857142, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.08333333333333333, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.08333333333333333, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.08333333333333333, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631578947368421, 0.02631578947368421, 0.0, 0.0, 0.0, 0.02631578947368421, 0.07894736842105263, 0.02631578947368421, 0.07894736842105263, 0.07894736842105263, 0.05263157894736842, 0.02631578947368421, 0.0, 0.05263157894736842, 0.02631578947368421, 0.0, 0.05263157894736842, 0.10526315789473684, 0.07894736842105263, 0.0, 0.0, 0.07894736842105263, 0.05263157894736842, 0.13157894736842105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.2727272727272727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.35, 0.0, 0.0, 0.0, 0.1, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.08333333333333333, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.03225806451612903, 0.0, 0.03225806451612903, 0.0967741935483871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3225806451612903, 0.0, 0.0967741935483871, 0.0, 0.12903225806451613, 0.1935483870967742, 0.0, 0.0, 0.03225806451612903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05555555555555555, 0.1111111111111111, 0.05555555555555555, 0.05555555555555555, 0.0, 0.16666666666666666, 0.1111111111111111, 0.16666666666666666, 0.1111111111111111, 0.0, 0.05555555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.2727272727272727, 0.0, 0.0, 0.0, 0.09090909090909091, 0.18181818181818182, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.0, 0.058823529411764705, 0.0, 0.058823529411764705, 0.0, 0.17647058823529413, 0.0, 0.0, 0.0, 0.058823529411764705, 0.35294117647058826, 0.0, 0.0, 0.0, 0.058823529411764705, 0.038461538461538464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038461538461538464, 0.0, 0.0, 0.0, 0.2692307692307692, 0.0, 0.0, 0.19230769230769232, 0.11538461538461539, 0.0, 0.038461538461538464, 0.0, 0.0, 0.19230769230769232, 0.07692307692307693, 0.038461538461538464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Markov: 0.548895796258597\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "INPUT_CSV  = \"dataset.csv\"\n",
        "OUTPUT_CSV = \"dataset_Similarity_calculation.csv\"\n",
        "N_ROWS     = 1000\n",
        "\n",
        "def clean(text: str) -> str:\n",
        "    \n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^0-9a-záéíóúüñ\\s]\", \" \", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def markov_vector(text: str, alphabet: list[str]) -> list[float]:\n",
        "    \n",
        "    idx = {c: i for i, c in enumerate(alphabet)}\n",
        "    size = len(alphabet)\n",
        "    mtx = np.zeros((size, size), dtype=float)\n",
        "    text = text.replace(\" \", \"\")\n",
        "    for a, b in zip(text, text[1:]):\n",
        "        if a in idx and b in idx:\n",
        "            mtx[idx[a], idx[b]] += 1\n",
        "    for i in range(size):\n",
        "        row_sum = mtx[i].sum()\n",
        "        if row_sum:\n",
        "            mtx[i] /= row_sum\n",
        "    return mtx.flatten().tolist()\n",
        "\n",
        "\n",
        "df = pd.read_csv(INPUT_CSV, nrows=N_ROWS)\n",
        "\n",
        "df[\"code1_clean\"] = df[\"code1\"].apply(clean)\n",
        "df[\"code2_clean\"] = df[\"code2\"].apply(clean)\n",
        "\n",
        "# Contenedores\n",
        "code1_vecBoW, code2_vecBoW, cosBoW = [], [], []\n",
        "code1_vecTFIDF, code2_vecTFIDF, cosTFIDF = [], [], []\n",
        "code1_vecMark, code2_vecMark, cosMark = [], [], []\n",
        "\n",
        "# Procesamiento\n",
        "for s1, s2 in zip(df[\"code1_clean\"], df[\"code2_clean\"]):\n",
        "    # Bag of Words\n",
        "    vocab, index_of = [], {}\n",
        "    for w in (s1.split() + s2.split()):\n",
        "        if w not in index_of:\n",
        "            index_of[w] = len(vocab)\n",
        "            vocab.append(w)\n",
        "    v1 = [0] * len(vocab)\n",
        "    v2 = [0] * len(vocab)\n",
        "    for w in s1.split():\n",
        "        v1[index_of[w]] += 1\n",
        "    for w in s2.split():\n",
        "        v2[index_of[w]] += 1\n",
        "    code1_vecBoW.append(v1)\n",
        "    code2_vecBoW.append(v2)\n",
        "    cosBoW.append(cosine_similarity([v1], [v2])[0, 0])\n",
        "\n",
        "    # TF-IDF\n",
        "    total1, total2 = sum(v1) or 1, sum(v2) or 1\n",
        "    tf1 = [c / total1 for c in v1]\n",
        "    tf2 = [c / total2 for c in v2]\n",
        "    idf = []\n",
        "    for w in vocab:\n",
        "        df_w = (1 if w in s1 else 0) + (1 if w in s2 else 0)\n",
        "        idf.append(math.log(2 / (df_w + 1)) + 1)\n",
        "    tfidf1 = [tf1[i] * idf[i] for i in range(len(vocab))]\n",
        "    tfidf2 = [tf2[i] * idf[i] for i in range(len(vocab))]\n",
        "    code1_vecTFIDF.append(tfidf1)\n",
        "    code2_vecTFIDF.append(tfidf2)\n",
        "    cosTFIDF.append(cosine_similarity([tfidf1], [tfidf2])[0, 0])\n",
        "\n",
        "    # Cadenas de Markov\n",
        "    alphabet = sorted(set(s1.replace(\" \", \"\") + s2.replace(\" \", \"\")))\n",
        "    vecM1 = markov_vector(s1, alphabet)\n",
        "    vecM2 = markov_vector(s2, alphabet)\n",
        "    code1_vecMark.append(vecM1)\n",
        "    code2_vecMark.append(vecM2)\n",
        "    cosMark.append(cosine_similarity([vecM1], [vecM2])[0, 0])\n",
        "\n",
        "# Guardar resultados\n",
        "df[\"code1_vecBoW\"] = code1_vecBoW\n",
        "df[\"code2_vecBoW\"] = code2_vecBoW\n",
        "df[\"cos_BOW\"] = cosBoW\n",
        "df[\"code1_vecTFIDF\"] = code1_vecTFIDF\n",
        "df[\"code2_vecTFIDF\"] = code2_vecTFIDF\n",
        "df[\"cos_TFID\"] = cosTFIDF\n",
        "df[\"code1_vecMark\"] = code1_vecMark\n",
        "df[\"code2_vecMark\"] = code2_vecMark\n",
        "df[\"cos_MARK\"] = cosMark\n",
        "\n",
        "df = df.drop(columns=[\"code1_clean\", \"code2_clean\"])\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(\"CSV guardado ->\", OUTPUT_CSV)\n",
        "\n",
        "# Verificación rápida\n",
        "print(\"Primera fila:\")\n",
        "print(\"BoW   :\", cosBoW[0])\n",
        "print(\"TF-IDF:\", cosTFIDF[0])\n",
        "print(\"Markov A:\", df[\"code1\"][0])\n",
        "print(\"Markov vector A:\", code1_vecMark[0])\n",
        "print(\"Markov B:\", df[\"code2\"][0])\n",
        "print(\"Markov vector B:\", code2_vecMark[0])\n",
        "print(\"Markov:\", cosMark[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
