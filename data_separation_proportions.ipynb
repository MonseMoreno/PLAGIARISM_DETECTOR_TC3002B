{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86591ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset = IR-Plag\n",
      "  Total pares en 'IR-Plag': 920\n",
      "    → train: 522 filas (56.74%)\n",
      "    → test : 134 filas (14.57%)\n",
      "    → val  : 264 filas (28.70%)\n",
      "\n",
      "Dataset = conplag_version_2\n",
      "  Total pares en 'conplag_version_2': 1820\n",
      "    → train: 1112 filas (61.10%)\n",
      "    → test : 356 filas (19.56%)\n",
      "    → val  : 352 filas (19.34%)\n",
      "\n",
      "Dataset = FIRE14\n",
      "  Total pares en 'FIRE14': 336\n",
      "    → train: 334 filas (99.40%)\n",
      "    → test : 0 filas (0.00%)\n",
      "    → val  : 2 filas (0.60%)\n",
      "\n",
      "=== Resumen global sobre TODOS los datasets ===\n",
      "Tamaños finales: train=1968 (63.98%), test=490 (15.93%), val=618 (20.09%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cargo el CSV original\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "total_rows = len(df)\n",
    "\n",
    "# Defino las funciones de unión-find (union by root, path compression)\n",
    "parent = {}\n",
    "def find(x):\n",
    "    parent.setdefault(x, x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(a, b):\n",
    "    ra, rb = find(a), find(b)\n",
    "    if ra != rb:\n",
    "        parent[rb] = ra\n",
    "\n",
    "# Preparar estructuras para ir guardando índices de train/test/val globales\n",
    "global_splits = {\"train\": [], \"test\": [], \"val\": []}\n",
    "global_split_sizes = {\"train\": 0, \"test\": 0, \"val\": 0}\n",
    "\n",
    "print()\n",
    "# Recorro CADA dataset por separado: IR-Plag, conplag_version_2, FIRE14\n",
    "for ds_label in df[\"dataset\"].unique():\n",
    "    subset = df[df[\"dataset\"] == ds_label].copy()\n",
    "    n_subset = len(subset)\n",
    "    if n_subset == 0:\n",
    "        continue\n",
    "\n",
    "    # Reinicio unión-find para este dataset\n",
    "    parent.clear()\n",
    "    for a, b in zip(subset[\"idcode1\"], subset[\"idcode2\"]):\n",
    "        union(a, b)\n",
    "\n",
    "    # Agrupo los índices de FILA por componente\n",
    "    comp_rows = defaultdict(list)\n",
    "    for idx in subset.index:\n",
    "        root = find(df.at[idx, \"idcode1\"])\n",
    "        comp_rows[root].append(idx)\n",
    "\n",
    "    # Mezclo componentes con semilla fija\n",
    "    components = list(comp_rows.items())\n",
    "    random.Random(42).shuffle(components)\n",
    "\n",
    "    # Objetivos 60/20/20 para este dataset\n",
    "    target_train = int(n_subset * 0.60)\n",
    "    target_test  = int(n_subset * 0.20)\n",
    "    target_val   = n_subset - target_train - target_test\n",
    "\n",
    "    local_splits = {\"train\": [], \"test\": [], \"val\": []}\n",
    "    local_sizes  = {\"train\": 0, \"test\": 0, \"val\": 0}\n",
    "\n",
    "    # Asigno cada componente al split cuyo \"remaining\" (objetivo - asignado) sea mayor\n",
    "    # Incluso si ya superamos el objetivo en todos, elegimos el menos negativo\n",
    "    for comp_id, rows in components:\n",
    "        remaining = {\n",
    "            \"train\": target_train - local_sizes[\"train\"],\n",
    "            \"test\":  target_test  - local_sizes[\"test\"],\n",
    "            \"val\":   target_val   - local_sizes[\"val\"],\n",
    "        }\n",
    "        # Escojo siempre el split con mayor remaining (incluso si es negativo)\n",
    "        chosen = max(remaining, key=lambda s: remaining[s])\n",
    "\n",
    "        local_splits[chosen].extend(rows)\n",
    "        local_sizes[chosen] += len(rows)\n",
    "\n",
    "    # Una vez asignados localmente, incorporo a los splits globales\n",
    "    for split_name in (\"train\", \"test\", \"val\"):\n",
    "        global_splits[split_name].extend(local_splits[split_name])\n",
    "        global_split_sizes[split_name] += local_sizes[split_name]\n",
    "\n",
    "    # Imprimo estadísticas para este dataset\n",
    "    train_count = local_sizes[\"train\"]\n",
    "    test_count  = local_sizes[\"test\"]\n",
    "    val_count   = local_sizes[\"val\"]\n",
    "\n",
    "    pct_train = (train_count / n_subset) * 100\n",
    "    pct_test  = (test_count  / n_subset) * 100\n",
    "    pct_val   = (val_count   / n_subset) * 100\n",
    "\n",
    "    print(f\"Dataset = {ds_label}\")\n",
    "    print(f\"  Total pares en '{ds_label}': {n_subset}\")\n",
    "    print(f\"    → train: {train_count} filas ({pct_train:.2f}%)\")\n",
    "    print(f\"    → test : {test_count} filas ({pct_test:.2f}%)\")\n",
    "    print(f\"    → val  : {val_count} filas ({pct_val:.2f}%)\\n\")\n",
    "\n",
    "# Construyo los DataFrames finales\n",
    "train_df = df.loc[global_splits[\"train\"]].reset_index(drop=True)\n",
    "test_df  = df.loc[global_splits[\"test\"] ].reset_index(drop=True)\n",
    "val_df   = df.loc[global_splits[\"val\"]  ].reset_index(drop=True)\n",
    "\n",
    "# Verificación de no solapamiento de códigos entre splits\n",
    "train_codes = set(train_df[\"idcode1\"]).union(train_df[\"idcode2\"])\n",
    "test_codes  = set(test_df[\"idcode1\"]).union(test_df[\"idcode2\"])\n",
    "val_codes   = set(val_df[\"idcode1\"]).union(val_df[\"idcode2\"])\n",
    "\n",
    "assert train_codes.isdisjoint(test_codes), \"¡Error: overlap train/test!\"\n",
    "assert train_codes.isdisjoint(val_codes),  \"¡Error: overlap train/val!\"\n",
    "assert test_codes.isdisjoint(val_codes),   \"¡Error: overlap test/val!\"\n",
    "\n",
    "# Guardo cada CSV\n",
    "train_df.to_csv(\"train.csv\", index=False, quoting=1)\n",
    "test_df.to_csv(\"test.csv\",  index=False, quoting=1)\n",
    "val_df.to_csv(\"val.csv\",   index=False, quoting=1)\n",
    "\n",
    "# Imprimo resumen global\n",
    "train_count = len(train_df)\n",
    "test_count  = len(test_df)\n",
    "val_count   = len(val_df)\n",
    "\n",
    "pct_train = (train_count / total_rows) * 100\n",
    "pct_test  = (test_count  / total_rows) * 100\n",
    "pct_val   = (val_count   / total_rows) * 100\n",
    "\n",
    "print(\"=== Resumen global sobre TODOS los datasets ===\")\n",
    "print(f\"Tamaños finales: train={train_count} ({pct_train:.2f}%), \"\n",
    "      f\"test={test_count} ({pct_test:.2f}%), val={val_count} ({pct_val:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a28b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Diagnóstico de la columna dataset` ===\n",
      "Valores únicos (hasta 50):\n",
      "['IR-Plag' 'conplag_version_2' 'FIRE14'] \n",
      "\n",
      "Filas que contienen 'IR-Plag' (exacto): 920\n",
      "Filas que contienen 'IR-Plag' (case-insensitive): 920\n",
      "===========================================\n",
      "\n",
      "Guardado 'plagiarism_levels.csv' con 134 filas y columna 'plagiarism_level'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"=== Diagnóstico de la columna dataset` ===\")\n",
    "print(\"Valores únicos (hasta 50):\")\n",
    "print(df[\"dataset\"].dropna().unique()[:50], \"\\n\")\n",
    "print(\"Filas que contienen 'IR-Plag' (exacto):\",\n",
    "      df[\"dataset\"].str.contains(\"IR-Plag\", na=False).sum())\n",
    "print(\"Filas que contienen 'IR-Plag' (case-insensitive):\",\n",
    "      df[\"dataset\"].str.contains(\"IR-Plag\", case=False, na=False).sum())\n",
    "print(\"===========================================\\n\")\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "ir_plag_df = df[df[\"dataset\"].str.contains(\"IR-Plag\", na=False)].copy()\n",
    "\n",
    "# Función para extraer el nivel de plagio de un único campo (idcode)\n",
    "def extract_level(idcode):\n",
    "    if pd.isna(idcode):\n",
    "        return -1\n",
    "    # Buscar patrones como \"-L1-\", \"-L2-\", ..., \"-L6-\"\n",
    "    match = re.search(r\"-L([1-6])-\", idcode)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    elif \"-NP-\" in idcode:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "# Función que primero intenta extraer de idcode2 y, si no hay resultado (> -1), usa idcode1\n",
    "def get_plagiarism_level(row):\n",
    "    level = extract_level(row.get(\"idcode2\", \"\"))\n",
    "    if level == -1:\n",
    "        level = extract_level(row.get(\"idcode1\", \"\"))\n",
    "    return level\n",
    "\n",
    "# Aplicar la función a cada fila\n",
    "ir_plag_df[\"plagiarism_level\"] = ir_plag_df.apply(get_plagiarism_level, axis=1)\n",
    "\n",
    "# Guardar el resultado\n",
    "ir_plag_df.to_csv(\"plagiarism_levels.csv\", index=False, quoting=1)\n",
    "\n",
    "print(f\"Guardado 'plagiarism_levels.csv' con {len(ir_plag_df)} filas y columna 'plagiarism_level'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
